{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd1a1ac9-a535-47bc-b3bb-0fc594fa96e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee2b71fd-6220-48e7-b797-114ada8e9ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainX, trainY), (testX, testY) = mnist.load_data()\n",
    "trainX = trainX.reshape(-1, 28 * 28).astype(\"float32\") / 255.0\n",
    "testX = testX.reshape(-1, 28 * 28).astype(\"float32\") / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95e70098-f0b2-44ba-8fcf-74edfeb70330",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100000 # total params\n",
    "L = 10 # hidden layer + output layer\n",
    "D = 0.3 # dropout probability\n",
    "F = 50 # number of neurons in first hidden layer\n",
    "LR = 0.0001 # learning rate\n",
    "initializer = tf.keras.initializers.GlorotUniform() # xavier initialization\n",
    "f = open(\"output_1.txt\", \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59233742-ea5b-4cf3-837b-4ff085065b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_6 (Batc  (None, 784)              3136      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " LAYER_1 (Dense)             (None, 50)                39250     \n",
      "                                                                 \n",
      " LAYER_2 (Dense)             (None, 85)                4335      \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 85)                0         \n",
      "                                                                 \n",
      " LAYER_3 (Dense)             (None, 85)                7310      \n",
      "                                                                 \n",
      " LAYER_4 (Dense)             (None, 85)                7310      \n",
      "                                                                 \n",
      " LAYER_5 (Dense)             (None, 85)                7310      \n",
      "                                                                 \n",
      " LAYER_6 (Dense)             (None, 85)                7310      \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 85)                0         \n",
      "                                                                 \n",
      " LAYER_7 (Dense)             (None, 85)                7310      \n",
      "                                                                 \n",
      " LAYER_8 (Dense)             (None, 85)                7310      \n",
      "                                                                 \n",
      " LAYER_9 (Dense)             (None, 94)                8084      \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 94)               376       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " OUTPUT_LAYER (Dense)        (None, 10)                950       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 99,991\n",
      "Trainable params: 98,235\n",
      "Non-trainable params: 1,756\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "3750/3750 - 9s - loss: 1.0702 - accuracy: 0.6426 - 9s/epoch - 3ms/step\n",
      "Epoch 2/5\n",
      "3750/3750 - 10s - loss: 0.4986 - accuracy: 0.8552 - 10s/epoch - 3ms/step\n",
      "Epoch 3/5\n",
      "3750/3750 - 8s - loss: 0.3926 - accuracy: 0.8881 - 8s/epoch - 2ms/step\n",
      "Epoch 4/5\n",
      "3750/3750 - 9s - loss: 0.3422 - accuracy: 0.9034 - 9s/epoch - 2ms/step\n",
      "Epoch 5/5\n",
      "3750/3750 - 9s - loss: 0.3002 - accuracy: 0.9177 - 9s/epoch - 2ms/step\n",
      "625/625 - 1s - loss: 0.1837 - accuracy: 0.9510 - 940ms/epoch - 2ms/step\n",
      "Accuracy of UniformNet: 0.9509999752044678\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##UniformNet\n",
    "# To get number of neurons in each layer solve the quadratic equation\n",
    "# 3136 + 785 * F + (F + 1)*J + (L-3) * J * (J+1) + 10 * (J+1) = N\n",
    "J = 85\n",
    "JL = 94\n",
    "UniformNet = tf.keras.Sequential()\n",
    "UniformNet.add(keras.layers.InputLayer(784))\n",
    "UniformNet.add(layers.BatchNormalization()) # BATCH NORMALIZATION\n",
    "UniformNet.add(layers.Dense(F, activation='relu', name='LAYER_1', kernel_initializer=initializer))\n",
    "UniformNet.add(layers.Dense(J, activation='relu', name='LAYER_2', kernel_initializer=initializer))\n",
    "UniformNet.add(layers.Dropout(D)) # DROPOUT\n",
    "UniformNet.add(layers.Dense(J, activation='relu', name='LAYER_3', kernel_initializer=initializer))\n",
    "UniformNet.add(layers.Dense(J, activation='relu', name='LAYER_4', kernel_initializer=initializer))\n",
    "UniformNet.add(layers.Dense(J, activation='relu', name='LAYER_5', kernel_initializer=initializer))\n",
    "UniformNet.add(layers.Dense(J, activation='relu', name='LAYER_6', kernel_initializer=initializer))\n",
    "UniformNet.add(layers.Dropout(D)) # DROPOUT\n",
    "UniformNet.add(layers.Dense(J, activation='relu', name='LAYER_7', kernel_initializer=initializer))\n",
    "UniformNet.add(layers.Dense(J, activation='relu', name='LAYER_8', kernel_initializer=initializer))\n",
    "UniformNet.add(layers.Dense(JL, activation='relu', name='LAYER_9', kernel_initializer=initializer))\n",
    "UniformNet.add(layers.BatchNormalization()) # BATCH NORMALIZATION\n",
    "UniformNet.add(layers.Dense(10, activation='softmax', name='OUTPUT_LAYER'))\n",
    "\n",
    "UniformNet.summary()\n",
    "\n",
    "UniformNet.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=LR),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "UniformNet.fit(trainX, trainY, batch_size=16, epochs=5, verbose=2)\n",
    "acc = UniformNet.evaluate(testX, testY, batch_size=16, verbose=2)[1]\n",
    "print(\"Accuracy of UniformNet: \"+str(acc)+'\\n')\n",
    "f.write(\"Accuracy of UniformNet: \"+str(acc)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98e7957d-9ade-4f2a-a610-bc27f8ffb659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_8 (Batc  (None, 784)              3136      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " LAYER_1 (Dense)             (None, 50)                39250     \n",
      "                                                                 \n",
      " LAYER_2 (Dense)             (None, 579)               29529     \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 579)               0         \n",
      "                                                                 \n",
      " LAYER_3 (Dense)             (None, 25)                14500     \n",
      "                                                                 \n",
      " LAYER_4 (Dense)             (None, 278)               7228      \n",
      "                                                                 \n",
      " LAYER_5 (Dense)             (None, 12)                3348      \n",
      "                                                                 \n",
      " LAYER_6 (Dense)             (None, 128)               1664      \n",
      "                                                                 \n",
      " LAYER_7 (Dense)             (None, 6)                 774       \n",
      "                                                                 \n",
      " LAYER_8 (Dense)             (None, 55)                385       \n",
      "                                                                 \n",
      " LAYER_9 (Dense)             (None, 3)                 168       \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 3)                12        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " OUTPUT_LAYER (Dense)        (None, 10)                40        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 100,034\n",
      "Trainable params: 98,460\n",
      "Non-trainable params: 1,574\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "3750/3750 - 9s - loss: 1.4285 - accuracy: 0.6246 - 9s/epoch - 2ms/step\n",
      "Epoch 2/5\n",
      "3750/3750 - 9s - loss: 0.9197 - accuracy: 0.8067 - 9s/epoch - 3ms/step\n",
      "Epoch 3/5\n",
      "3750/3750 - 10s - loss: 0.6597 - accuracy: 0.8833 - 10s/epoch - 3ms/step\n",
      "Epoch 4/5\n",
      "3750/3750 - 8s - loss: 0.5009 - accuracy: 0.9159 - 8s/epoch - 2ms/step\n",
      "Epoch 5/5\n",
      "3750/3750 - 8s - loss: 0.4197 - accuracy: 0.9247 - 8s/epoch - 2ms/step\n",
      "625/625 - 1s - loss: 0.2630 - accuracy: 0.9538 - 672ms/epoch - 1ms/step\n",
      "Accuracy of PyramidNet: 0.9538000226020813\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PyramidNet\n",
    "# J's denote the neurons in each layer they are calculated to make parameters ROUGHLY half every layer\n",
    "# do not consider number of parameters of batch normalization layers and initial layers\n",
    "\n",
    "J1 = 579\n",
    "J2 = (F + 1)  * J1  // ((J1 + 1) * 2)\n",
    "J3 = (J1 + 1) * J2 // ((J2 + 1) * 2)\n",
    "J4 = (J2 + 1) * J3 // ((J3 + 1) * 2)\n",
    "J5 = (J3 + 1) * J4 // ((J4 + 1) * 2)\n",
    "J6 = (J4 + 1) * J5 // ((J5 + 1) * 2)\n",
    "J7 = (J5 + 1) * J6 // ((J6 + 1) * 2)\n",
    "J8 = (J6 + 1) * J7 // ((J7 + 1) * 2)\n",
    "\n",
    "PyramidNet = tf.keras.Sequential()\n",
    "PyramidNet.add(keras.layers.InputLayer(784))\n",
    "PyramidNet.add(layers.BatchNormalization()) # BATCH NORMALIZATION\n",
    "PyramidNet.add(layers.Dense(F, activation='relu', name='LAYER_1', kernel_initializer=initializer))\n",
    "PyramidNet.add(layers.Dense(J1, activation='relu', name='LAYER_2', kernel_initializer=initializer))\n",
    "PyramidNet.add(layers.Dropout(D)) # DROPOUT\n",
    "PyramidNet.add(layers.Dense(J2, activation='relu', name='LAYER_3', kernel_initializer=initializer))\n",
    "PyramidNet.add(layers.Dense(J3, activation='relu', name='LAYER_4', kernel_initializer=initializer))\n",
    "PyramidNet.add(layers.Dense(J4, activation='relu', name='LAYER_5', kernel_initializer=initializer))\n",
    "PyramidNet.add(layers.Dense(J5, activation='relu', name='LAYER_6', kernel_initializer=initializer))\n",
    "PyramidNet.add(layers.Dense(J6, activation='relu', name='LAYER_7', kernel_initializer=initializer))\n",
    "PyramidNet.add(layers.Dense(J7, activation='relu', name='LAYER_8', kernel_initializer=initializer))\n",
    "PyramidNet.add(layers.Dense(J8, activation='relu', name='LAYER_9', kernel_initializer=initializer))\n",
    "PyramidNet.add(layers.BatchNormalization()) # BATCH NORMALIZATION\n",
    "PyramidNet.add(layers.Dense(10, activation='softmax', name='OUTPUT_LAYER'))\n",
    "\n",
    "PyramidNet.summary()\n",
    "\n",
    "PyramidNet.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=LR),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "PyramidNet.fit(trainX, trainY, batch_size=16, epochs=5, verbose=2)\n",
    "acc = PyramidNet.evaluate(testX, testY, batch_size=16, verbose=2)[1]\n",
    "print(\"Accuracy of PyramidNet: \"+str(acc)+'\\n')\n",
    "f.write(\"Accuracy of PyramidNet: \"+str(acc)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3dc4441e-b467-49d3-b245-129d04c61afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_10 (Bat  (None, 784)              3136      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " LAYER_1 (Dense)             (None, 50)                39250     \n",
      "                                                                 \n",
      " LAYER_2 (Dense)             (None, 5)                 255       \n",
      "                                                                 \n",
      " LAYER_3 (Dense)             (None, 85)                510       \n",
      "                                                                 \n",
      " LAYER_4 (Dense)             (None, 11)                946       \n",
      "                                                                 \n",
      " LAYER_5 (Dense)             (None, 157)               1884      \n",
      "                                                                 \n",
      " LAYER_6 (Dense)             (None, 23)                3634      \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 23)                0         \n",
      "                                                                 \n",
      " LAYER_7 (Dense)             (None, 302)               7248      \n",
      "                                                                 \n",
      " LAYER_8 (Dense)             (None, 47)                14241     \n",
      "                                                                 \n",
      " LAYER_9 (Dense)             (None, 466)               22368     \n",
      "                                                                 \n",
      " batch_normalization_11 (Bat  (None, 466)              1864      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " OUTPUT_LAYER (Dense)        (None, 10)                4670      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 100,006\n",
      "Trainable params: 97,506\n",
      "Non-trainable params: 2,500\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "3750/3750 - 9s - loss: 0.9585 - accuracy: 0.6700 - 9s/epoch - 2ms/step\n",
      "Epoch 2/5\n",
      "3750/3750 - 9s - loss: 0.5118 - accuracy: 0.8513 - 9s/epoch - 2ms/step\n",
      "Epoch 3/5\n",
      "3750/3750 - 9s - loss: 0.4151 - accuracy: 0.8829 - 9s/epoch - 2ms/step\n",
      "Epoch 4/5\n",
      "3750/3750 - 8s - loss: 0.3529 - accuracy: 0.9012 - 8s/epoch - 2ms/step\n",
      "Epoch 5/5\n",
      "3750/3750 - 8s - loss: 0.3152 - accuracy: 0.9126 - 8s/epoch - 2ms/step\n",
      "625/625 - 1s - loss: 0.2530 - accuracy: 0.9321 - 679ms/epoch - 1ms/step\n",
      "Accuracy of InvPyramidNet: 0.9320999979972839\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#InvPyramidNet\n",
    "# J's denote the neurons in each layer they are calculated to make parameters ROUGHLY double every layer \n",
    "# do not consider number of parameters of batch normalization layers and initial layers\n",
    "J1 = 5\n",
    "J2 = 2 * (F + 1)  * J1  // (J1 + 1)\n",
    "J3 = 2 * (J1 + 1) * J2 // (J2 + 1)\n",
    "J4 = 2 * (J2 + 1) * J3 // (J3 + 1)\n",
    "J5 = 2 * (J3 + 1) * J4 // (J4 + 1)\n",
    "J6 = 2 * (J4 + 1) * J5 // (J5 + 1)\n",
    "J7 = 2 * (J5 + 1) * J6 // (J6 + 1)\n",
    "JL = 466\n",
    "\n",
    "InvPyramidNet = tf.keras.Sequential()\n",
    "InvPyramidNet.add(keras.layers.InputLayer(784))\n",
    "InvPyramidNet.add(layers.BatchNormalization()) # BATCH NORMALIZATION\n",
    "InvPyramidNet.add(layers.Dense(F, activation='relu', name='LAYER_1', kernel_initializer=initializer))\n",
    "InvPyramidNet.add(layers.Dense(J1, activation='relu', name='LAYER_2', kernel_initializer=initializer))\n",
    "InvPyramidNet.add(layers.Dense(J2, activation='relu', name='LAYER_3', kernel_initializer=initializer))\n",
    "InvPyramidNet.add(layers.Dense(J3, activation='relu', name='LAYER_4', kernel_initializer=initializer))\n",
    "InvPyramidNet.add(layers.Dense(J4, activation='relu', name='LAYER_5', kernel_initializer=initializer))\n",
    "InvPyramidNet.add(layers.Dense(J5, activation='relu', name='LAYER_6', kernel_initializer=initializer))\n",
    "InvPyramidNet.add(layers.Dropout(D)) # DROPOUT\n",
    "InvPyramidNet.add(layers.Dense(J6, activation='relu', name='LAYER_7', kernel_initializer=initializer))\n",
    "InvPyramidNet.add(layers.Dense(J7, activation='relu', name='LAYER_8', kernel_initializer=initializer))\n",
    "InvPyramidNet.add(layers.Dense(JL, activation='relu', name='LAYER_9', kernel_initializer=initializer))\n",
    "InvPyramidNet.add(layers.BatchNormalization()) # BATCH NORMALIZATION\n",
    "InvPyramidNet.add(layers.Dense(10, activation='softmax', name='OUTPUT_LAYER'))\n",
    "\n",
    "InvPyramidNet.summary()\n",
    "\n",
    "InvPyramidNet.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=LR),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "InvPyramidNet.fit(trainX, trainY, batch_size=16, epochs=5, verbose=2)\n",
    "acc = InvPyramidNet.evaluate(testX, testY, batch_size=16, verbose=2)[1]\n",
    "print(\"Accuracy of InvPyramidNet: \"+str(acc)+'\\n')\n",
    "f.write(\"Accuracy of InvPyramidNet: \"+str(acc)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f404f59a-6401-4dc3-85d0-95494b116bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393796af-4654-4037-846f-2257f3e31c2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
